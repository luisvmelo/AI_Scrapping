"""
Scraper para Futurepedia
URL: https://www.futurepedia.io
"""

import json
import re
import time
from bs4 import BeautifulSoup
from typing import List
from datetime import datetime
from .common import BaseScraper, AITool

class FuturepediaScraper(BaseScraper):
    """Scraper para futurepedia.io"""
    
    def __init__(self):
        super().__init__("futurepedia", "https://www.futurepedia.io")
    
    def scrape(self, max_tools=50) -> List[AITool]:
        """Scrape das ferramentas de AI"""
        tools = []
        
        # Skip main page scraping and go directly to categories for efficiency
        # Main page doesn't have individual tool links
        
        # Tenta buscar em categorias espec√≠ficas (limitado)
        category_tools = self._scrape_categories(max_tools=max_tools)
        tools.extend(category_tools)
        
        return tools[:max_tools]
    
    def _scrape_tools_page(self) -> List[AITool]:
        """Scrape da p√°gina principal de ferramentas, priorizando populares"""
        tools = []
        
        # URLs priorizando p√°ginas populares/famosas primeiro
        priority_urls = [
            f"{self.base_url}/ai-tools?sort=popular",     # Mais populares
            f"{self.base_url}/ai-tools?sort=trending",    # Tend√™ncias
            f"{self.base_url}/ai-tools?sort=featured",    # Destacados
            f"{self.base_url}/ai-tools?sort=top",         # Top rated
            f"{self.base_url}/ai-tools",                  # P√°gina geral
            f"{self.base_url}/popular",                   # P√°gina popular direta
            f"{self.base_url}/trending",                  # P√°gina trending direta
            f"{self.base_url}/featured"                   # P√°gina featured direta
        ]
        
        for tools_url in priority_urls:
            try:
                print(f"üîç Fazendo scraping da p√°gina de ferramentas: {tools_url}")
                
                response = self.get_page(tools_url)
                if not response:
                    print(f"‚ùå Erro ao acessar {tools_url}")
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Busca por links de ferramentas (v√°rios padr√µes)
                tool_links = soup.select('a[href*="/tool/"]')
                
                print(f"üìä Encontrados {len(tool_links)} links de ferramentas na p√°gina principal")
                
                # Processa links √∫nicos para evitar duplicatas (com limite)
                seen_tools = set()
                for i, link in enumerate(tool_links[:50]):  # Limite de 50 links por p√°gina
                    try:
                        href = link.get('href', '')
                        if href in seen_tools:
                            continue
                        seen_tools.add(href)
                        
                        tool = self._parse_tool_card(link, i)
                        if tool and tool.name != "Unknown Tool" and len(tool.name) > 2:
                            tools.append(tool)
                    except Exception as e:
                        print(f"‚ùå Erro ao processar link {i}: {e}")
                        continue
                
                print(f"‚úÖ P√°gina principal: Scraped {len(tools)} ferramentas")
                
            except Exception as e:
                print(f"‚ùå Erro geral no scraping da p√°gina principal: {e}")
        
        return tools
    
    def _scrape_categories(self, max_tools=50) -> List[AITool]:
        """Scrape de p√°ginas de categorias espec√≠ficas com rota√ß√£o eficiente"""
        tools = []
        
        # Estrat√©gia: Diversificar entre categorias ao inv√©s de esgotar uma categoria
        # Apenas categorias v√°lidas (404 removidas)
        categories = [
            ("business", 1517),      # AI Business Tools
            ("productivity", 605),   # AI Productivity Tools  
            ("image", 298),          # AI Image Tools
            ("code", 187),           # AI Code Tools
            ("video", 169),          # AI Video Tools
            ("art", 117)             # AI Art Generators
        ]
        
        global_seen_tools = set()  # Evita duplicatas entre categorias
        
        # Nova estrat√©gia: Rota√ß√£o entre categorias
        # Ao inv√©s de esgotar uma categoria, pega algumas ferramentas de cada
        tools_per_category = max(1, max_tools // len(categories))  # Distribui igualmente
        max_pages_per_category = 10  # M√°ximo 10 p√°ginas por categoria
        
        for category, total_tools in categories:
            # Stop if we have enough tools
            if len(tools) >= max_tools:
                break
                
            try:
                remaining_needed = max_tools - len(tools)
                target_for_category = min(tools_per_category, remaining_needed)
                print(f"üîç Scraping categoria: {category} (target: {target_for_category} tools)")
                category_tools = []
                
                # Scrape p√°ginas da categoria (com limite r√≠gido)
                page = 1
                while page <= max_pages_per_category and len(category_tools) < target_for_category:
                    try:
                        # URL com pagina√ß√£o
                        if page == 1:
                            category_url = f"{self.base_url}/ai-tools/{category}"
                        else:
                            category_url = f"{self.base_url}/ai-tools/{category}?page={page}"
                        
                        print(f"   üìÑ P√°gina {page}: {category_url}")
                        
                        response = self.get_page(category_url)
                        if not response:
                            print(f"   ‚ùå Erro ao acessar p√°gina {page} da categoria {category}")
                            break
                        
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # Busca por links de ferramentas (v√°rios padr√µes)
                        tool_links = soup.select('a[href*="/tool/"]')
                        
                        if not tool_links:
                            print(f"   ‚ö†Ô∏è Nenhuma ferramenta encontrada na p√°gina {page} - FIM da categoria")
                            break
                        
                        print(f"   üìä Encontrados {len(tool_links)} links na p√°gina {page}")
                        
                        page_tools = []
                        for i, link in enumerate(tool_links):
                            try:
                                # Stop if we have enough tools for this category
                                if len(category_tools) + len(page_tools) >= target_for_category:
                                    break
                                    
                                href = link.get('href', '')
                                if href in global_seen_tools:
                                    continue
                                global_seen_tools.add(href)
                                
                                tool = self._parse_tool_card(link, len(category_tools) + i, category)
                                if tool and tool.name != "Unknown Tool" and len(tool.name) > 2:
                                    page_tools.append(tool)
                            except Exception as e:
                                continue
                        
                        category_tools.extend(page_tools)
                        print(f"   ‚úÖ P√°gina {page}: {len(page_tools)} ferramentas extra√≠das")
                        
                        # Check if we have enough tools for this category
                        if len(category_tools) >= target_for_category:
                            print(f"   üéØ Reached category target of {target_for_category} tools")
                            break
                        
                        # Incrementa p√°gina e pausa entre p√°ginas
                        page += 1
                        time.sleep(2)
                            
                    except Exception as e:
                        print(f"   ‚ùå Erro na p√°gina {page} da categoria {category}: {e}")
                        page += 1  # Incrementa mesmo em caso de erro
                        if page > max_pages_per_category:  # Limite de seguran√ßa para evitar loop infinito
                            print(f"   üõë Limite de seguran√ßa atingido ({max_pages_per_category} p√°ginas) para categoria {category}")
                            break
                        continue
                
                tools.extend(category_tools)
                print(f"‚úÖ Categoria {category}: {len(category_tools)} ferramentas totais")
                
                # Pausa entre categorias
                time.sleep(3)
                
            except Exception as e:
                print(f"‚ùå Erro na categoria {category}: {e}")
                continue
        
        return tools
    
    def _parse_tool_card(self, link_element, index: int, category: str = "") -> AITool:
        """Extrai dados de uma ferramenta do card link do Futurepedia"""
        
        # URL da ferramenta
        href = link_element.get('href', '')
        if not ('/tool/' in href):
            return None
        
        # Torna URL absoluta se necess√°rio
        if href.startswith('/'):
            tool_url = self.base_url + href
        else:
            tool_url = href
        
        # Nome da ferramenta - busca no elemento pai e nos elementos pr√≥ximos
        name = ""
        
        # Primeiro, tenta encontrar o nome no elemento pai
        parent_element = link_element.parent
        if parent_element:
            # Busca por headings no pai
            name_elem = (
                parent_element.select_one('h1, h2, h3, h4, h5, h6') or
                parent_element.select_one('[class*="title"]') or
                parent_element.select_one('[class*="name"]')
            )
            
            if name_elem:
                name = name_elem.get_text().strip()
            else:
                # Se n√£o achou heading, extrai o nome do texto do pai
                parent_text = parent_element.get_text().strip()
                if parent_text:
                    # O nome geralmente √© a primeira palavra/frase antes de "Rated"
                    lines = parent_text.split('Rated')[0].strip()
                    name = lines.split('\n')[0].strip()
        
        # Se ainda n√£o encontrou, tenta o img alt text
        if not name:
            img_elem = link_element.select_one('img')
            if img_elem and img_elem.get('alt'):
                alt_text = img_elem.get('alt')
                # Remove " logo" do final se estiver presente
                name = alt_text.replace(' logo', '').strip()
        
        # Limpa e valida o nome
        if name:
            name = name.replace('\n', ' ').replace('\t', ' ')
            name = ' '.join(name.split())[:100]
        
        if not name or len(name) < 2:
            name = f"Futurepedia Tool {index}"
        
        # Descri√ß√£o - busca no elemento pai e elementos pr√≥ximos
        description = ""
        
        # Tenta encontrar descri√ß√£o no elemento pai ou elementos irm√£os
        if parent_element:
            desc_elem = (
                parent_element.select_one('p') or
                parent_element.select_one('[class*="desc"]') or
                parent_element.select_one('[class*="summary"]') or
                parent_element.select_one('div[class*="text"]')
            )
            
            if desc_elem:
                description = desc_elem.get_text().strip()
        
        # Se n√£o encontrou, procura no pr√≥ximo elemento irm√£o do pai
        if not description and parent_element and parent_element.next_sibling:
            sibling = parent_element.next_sibling
            if hasattr(sibling, 'select_one'):
                desc_elem = (
                    sibling.select_one('p') or
                    sibling.select_one('[class*="desc"]') or
                    sibling.select_one('[class*="summary"]')
                )
                if desc_elem:
                    description = desc_elem.get_text().strip()
        
        # Limpa a descri√ß√£o
        if description:
            description = description.replace('\n', ' ').replace('\t', ' ')
            description = ' '.join(description.split())[:500]
        
        # Busca por rating (padr√£o: "Rated X.X out of 5")
        rating_text = ""
        if parent_element:
            rating_text = parent_element.get_text().lower()
        else:
            rating_text = link_element.get_text().lower()
            
        popularity = 50  # valor padr√£o
        
        if 'rated' in rating_text and 'out of 5' in rating_text:
            try:
                # Extrai o rating do texto
                rating_match = re.search(r'rated\s+(\d+\.?\d*)\s+out\s+of\s+5', rating_text)
                if rating_match:
                    rating = float(rating_match.group(1))
                    popularity = (rating / 5) * 100  # Converte para escala 0-100
            except:
                pass
        
        # Busca por informa√ß√µes de pre√ßo MELHORADAS
        if parent_element:
            price_text = parent_element.get_text().lower()
        else:
            price_text = link_element.get_text().lower()
            
        price = "Unknown"
        
        # Busca por pre√ßos espec√≠ficos primeiro ($X/month, $X/year)
        price_patterns = [
            r'\$(\d+(?:,\d+)?(?:\.\d{2})?)\s*(?:/|\s+per\s+)?(?:month|mo|monthly)',
            r'\$(\d+(?:,\d+)?(?:\.\d{2})?)\s*(?:/|\s+per\s+)?(?:year|yr|yearly|annually)',
            r'\$(\d+(?:,\d+)?(?:\.\d{2})?)\s*(?:/|\s+per\s+)?(?:week|weekly)',
            r'\$(\d+(?:,\d+)?(?:\.\d{2})?)\s*(?:/|\s+one.?time|once)',
            r'(\d+(?:,\d+)?(?:\.\d{2})?)\s*(?:usd|dollars?)\s*(?:/|\s+per\s+)?(?:month|mo)',
        ]
        
        for pattern in price_patterns:
            match = re.search(pattern, price_text, re.IGNORECASE)
            if match:
                amount = match.group(1).replace(',', '')
                if 'month' in pattern or '/mo' in pattern:
                    price = f"${amount}/month"
                elif 'year' in pattern or '/yr' in pattern:
                    price = f"${amount}/year"
                elif 'week' in pattern:
                    price = f"${amount}/week"
                elif 'one.?time' in pattern or 'once' in pattern:
                    price = f"${amount} one-time"
                else:
                    price = f"${amount}/month"  # Default to monthly
                break
        
        # Se n√£o encontrou pre√ßo espec√≠fico, busca por categorias
        if price == "Unknown":
            if 'free trial' in price_text:
                price = "Free Trial"
            elif 'freemium' in price_text:
                price = "Freemium"
            elif 'free' in price_text and 'trial' not in price_text:
                price = "Free"
            elif any(word in price_text for word in ['paid', 'premium', 'subscription', 'plans']):
                price = "Paid"
            elif '$' in price_text:
                # Busca qualquer valor em d√≥lar
                dollar_match = re.search(r'\$(\d+(?:,\d+)?(?:\.\d{2})?)', price_text)
                if dollar_match:
                    amount = dollar_match.group(1)
                    price = f"${amount}"
        
        # Busca por tags de categoria (podem estar em spans ou divs com # prefix)
        categories = []
        
        # Busca tags no elemento pai tamb√©m
        search_elements = [link_element]
        if parent_element:
            search_elements.append(parent_element)
        
        for search_elem in search_elements:
            tag_elements = search_elem.select('span, div')
            for tag_elem in tag_elements:
                tag_text = tag_elem.get_text().strip()
                if tag_text.startswith('#'):
                    # Remove o # e adiciona √† lista
                    clean_tag = tag_text[1:].strip()
                    if clean_tag and clean_tag not in categories:
                        categories.append(clean_tag)
        
        # Adiciona categoria da URL se dispon√≠vel
        if category:
            categories.append(category)
        
        # Se n√£o tem categorias, infere do texto
        if not categories:
            categories = self._infer_categories_from_text(name + " " + description)
        
        # Ajusta popularidade baseada na posi√ß√£o
        position_bonus = max(0, 20 - index)
        popularity = min(100, popularity + position_bonus)
        
        # ID √∫nico baseado na URL da ferramenta
        tool_slug = href.split('/tool/')[-1]  # Funciona para URLs completas e relativas
        ext_id = f"fp_{tool_slug}"
        
        # Extrai informa√ß√µes adicionais
        # Logo URL
        logo_url = self.extract_logo_url(parent_element or link_element, tool_url)
        
        # Platform info
        platform = self.extract_platform_info(parent_element or link_element, description)
        
        # Features
        features = self.extract_features(parent_element or link_element, description)
        
        # Rank baseado na posi√ß√£o na p√°gina (ferramentas no topo s√£o mais relevantes)
        rank = index + 1
        
        # Upvotes - tenta extrair de elementos de rating/votes
        upvotes = None
        upvotes_patterns = [r'(\d+)\s*(?:votes?|upvotes?)', r'(\d+)\s*üëç', r'(\d+)\s*likes?']
        element_text = (parent_element.get_text() if parent_element else link_element.get_text())
        upvotes = self.extract_numeric_value(element_text, upvotes_patterns)
        
        # Monthly users - busca por padr√µes de usu√°rios
        monthly_users = None
        users_patterns = [
            r'(\d+(?:,\d+)*)\s*(?:monthly\s+)?users?',
            r'(\d+(?:,\d+)*)\s*people\s+use',
            r'(\d+(?:,\d+)*)\s*active\s+users?'
        ]
        monthly_users = self.extract_numeric_value(element_text, users_patterns)
        
        # Editor score baseado no rating do Futurepedia
        editor_score = None
        if popularity and popularity > 0:
            # Converte popularidade (0-100) para score (0-10)
            editor_score = round(popularity / 10, 1)
        
        # Maturity - infere do texto
        maturity = None
        maturity_text = element_text.lower()
        if 'beta' in maturity_text:
            maturity = 'beta'
        elif 'alpha' in maturity_text:
            maturity = 'alpha'
        elif any(term in maturity_text for term in ['stable', 'production', 'ga', 'v1', 'version 1']):
            maturity = 'stable'
        
        # Classifica√ß√£o de dom√≠nio
        macro_domain = self.classify_domain(categories, description)
        
        return AITool(
            ext_id=ext_id,
            name=name,
            description=description,
            price=price,
            popularity=float(popularity),
            categories=categories,
            source=self.source_name,
            macro_domain=macro_domain,
            # Enhanced fields
            url=tool_url,
            logo_url=logo_url,
            rank=rank,
            upvotes=upvotes,
            monthly_users=monthly_users,
            editor_score=editor_score,
            maturity=maturity,
            platform=platform,
            features=features,
            last_scraped=datetime.now()
        )
    
    def _infer_categories_from_text(self, text: str) -> List[str]:
        """Infere categorias baseado no texto"""
        text_lower = text.lower()
        categories = []
        
        # Mapeamento de palavras-chave para categorias espec√≠ficas do Futurepedia
        keyword_map = {
            "writing": ["write", "writing", "text", "content", "copywriting", "blog", "article", "essay"],
            "chatbot": ["chat", "conversation", "bot", "assistant", "talk", "dialogue"],
            "image": ["image", "photo", "picture", "visual", "graphic", "art", "generate"],
            "video": ["video", "film", "movie", "clip", "animation"],
            "audio": ["audio", "music", "sound", "voice", "podcast", "speech"],
            "code": ["code", "programming", "developer", "api", "coding", "development"],
            "design": ["design", "ui", "ux", "creative", "prototype"],
            "productivity": ["productivity", "organize", "task", "management", "workflow"],
            "research": ["research", "analysis", "data", "insight", "study"],
            "marketing": ["marketing", "seo", "social", "campaign", "advertising"],
            "education": ["education", "learning", "teach", "course", "training"],
            "business": ["business", "startup", "entrepreneur", "finance", "sales"]
        }
        
        for category, keywords in keyword_map.items():
            if any(keyword in text_lower for keyword in keywords):
                categories.append(category)
        
        # Se n√£o encontrou nenhuma categoria, adiciona categorias padr√£o
        if not categories:
            categories = ["ai-tool", "general"]
        
        return categories


def scrape_futurepedia() -> List[AITool]:
    """Fun√ß√£o de conveni√™ncia para fazer scraping do Futurepedia"""
    scraper = FuturepediaScraper()
    return scraper.scrape()